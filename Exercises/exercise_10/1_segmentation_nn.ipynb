{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Segmentation \n",
    "============\n",
    "\n",
    "In this exercise you are going to work on a computer vision task called semantic segmentation. In comparison to image classification the goal is not to classify an entire image but each of its pixels separately. This implies that the  output of the network is not a single scalar but a segmentation with the same shape as the input image. Think about why you should rather use convolutional than fully-connected layers for this task!\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/d10b897e15344334e449104a824aff6c29125dc2/687474703a2f2f63616c76696e2e696e662e65642e61632e756b2f77702d636f6e74656e742f75706c6f6164732f646174612f636f636f7374756666646174617365742f636f636f73747566662d6578616d706c65732e706e67\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_10) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_10'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up PyTorch environment in colab\n",
    "- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n",
    "- Uncomment the following cell if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: install correct libraries in google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !python -m pip install tensorboard==2.8.0\n",
    "# !python -m pip install pytorch-lightning==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# For google colab\n",
    "# !python -m pip install pytorch-lightning==1.6.0 > /dev/null\n",
    "\n",
    "# For anaconda/regular python\n",
    "# !{sys.executable} -m pip install pytorch-lightning==1.6.0 > /dev/null\n",
    "# 1. Preparation\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from exercise_code.data.segmentation_dataset import SegmentationData, label_img_to_rgb\n",
    "from exercise_code.data.download_utils import download_dataset\n",
    "from exercise_code.util import visualizer, save_model\n",
    "from exercise_code.util.Util import checkSize, checkParams, test\n",
    "from exercise_code.networks.segmentation_nn import SegmentationNN, DummySegmentationModel\n",
    "from exercise_code.tests import test_seg_nn\n",
    "#set up default cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorBoard\n",
    "In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! Throughout this notebook, feel free to add further logs or visualizations to your TensorBoard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data\n",
    "\n",
    "#### MSRC-v2 Segmentation Dataset\n",
    "\n",
    "The MSRC v2 dataset is an extension of the MSRC v1 dataset from Microsoft Research in Cambridge. It contains *591* images and *23* object classes with accurate pixel-wise labeled images. \n",
    "\n",
    "\n",
    "\n",
    "The image ids are stored in the txt file `train.txt`, `val.txt`, `test.txt`. The dataloader will read the image id in the txt file and fetch the corresponding input and target images from the image folder. \n",
    "<img src='images/input_target.png'/>\n",
    "\n",
    "\n",
    "\n",
    "As you can see in `exercise_code/data/segmentation_dataset.py`, each segmentation label has its corresponding RGB value stored in the `SEG_LABELS_LIST`. The label `void` means `unlabeled`, and it is displayed as black `\"rgb_values\": [0, 0, 0]` in the target image. The target image pixels will be labeled based on its color using `SEG_LABELS_LIST`.\n",
    "\n",
    "```python\n",
    "                SEG_LABELS_LIST = [\n",
    "                {\"id\": -1, \"name\": \"void\",       \"rgb_values\": [0,   0,    0]},\n",
    "                {\"id\": 0,  \"name\": \"building\",   \"rgb_values\": [128, 0,    0]},\n",
    "                {\"id\": 1,  \"name\": \"grass\",      \"rgb_values\": [0,   128,  0]},\n",
    "                {\"id\": 2,  \"name\": \"tree\",       \"rgb_values\": [128, 128,  0]},\n",
    "                {\"id\": 3,  \"name\": \"cow\",        \"rgb_values\": [0,   0,    128]},\n",
    "                {\"id\": 4,  \"name\": \"horse\",      \"rgb_values\": [128, 0,    128]},\n",
    "                {\"id\": 5,  \"name\": \"sheep\",      \"rgb_values\": [0,   128,  128]},\n",
    "                ...]    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h3>Note: The label <code>void</code></h3>\n",
    "    <p>Pixels with the label <code>void</code> should neither be considered in your loss nor in the accuracy of your segmentation. See implementation for details.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = 'https://i2dl.vc.in.tum.de/static/data/segmentation_data.zip'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, 'datasets','segmentation')\n",
    "\n",
    "\n",
    "download_dataset(\n",
    "    url=download_url,\n",
    "    data_dir=data_root,\n",
    "    dataset_zip_name='segmentation_data.zip',\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "train_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/train.txt')\n",
    "val_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/val.txt')\n",
    "test_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to implement data augmentation methods, make yourself familiar with the segmentation dataset and how we implemented the `SegmentationData` class in `exercise_code/data/segmentation_dataset.py`. Furthermore, you can check the original label description in `datasets/segmentation/segmentation_data/info.html`.\n",
    "\n",
    "For now, let's look at a few samples of our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Img size: \", train_data[0][0].size())\n",
    "print(\"Segmentation size: \", train_data[0][1].size())\n",
    "\n",
    "num_example_imgs = 4\n",
    "plt.figure(figsize=(10, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n",
    "    plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n",
    "    plt.imshow(label_img_to_rgb(target.numpy()))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that the dataset is quite small in comparison to our previous datasets, e.g., for CIFAR10 we had ten thousands of images while we only have 276 training images in this case. In addition, the task is much more difficult than a \"simple 10 class classification\", as we have to assign a label to each pixel! What's more, the images are much bigger as we are now considering images of size 240x240 instead of 32x32. \n",
    "\n",
    "That means that you shouldn't expect our networks to perform very well, so don't be too disappointed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Semantic Segmentation \n",
    "\n",
    "## Dummy Model\n",
    "\n",
    "In `exercise_code/networks/segmentation_nn.py` we define a naive `DummySegmentationModel`, which always predicts the scores of segmentation labels of the first image. Let's try it on a few images and visualize the outputs using the `visualizer` we provide. The `visualizer` takes in the model and dataset, and visualizes the first four (Input, Target, Prediction) pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dummy_model = DummySegmentationModel(target_image=train_data[0][1])\n",
    "\n",
    "# Visualization function\n",
    "visualizer(dummy_model, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can use the visualizer function in your training scenario to print out your model predictions on a regular basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Metrics\n",
    "The loss function for the task of image segmentation is a pixel-wise cross entropy loss. This loss examines each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector. \n",
    "<img src='images/loss_img.png' width=80% height=80%/>\n",
    "source: https://www.jeremyjordan.me/semantic-segmentation/\n",
    "\n",
    "Up until now we only used the default loss function (`nn.CrossEntropyLoss`) in our solvers. However, In order to ignore the `unlabeled` pixels for the computation of our loss, we have to use a customized version of the loss for the initialization of our segmentation solver. The `ignore_index` argument of the loss can be used to filter the `unlabeled` pixels and computes the loss only over remaining pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "\n",
    "for (inputs, targets) in train_data[0:4]:\n",
    "    inputs, targets = inputs, targets\n",
    "    outputs = dummy_model(inputs.unsqueeze(0))\n",
    "    losses = loss_func(outputs, targets.unsqueeze(0))\n",
    "    print(losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Non-zero loss for the first sample</h3> \n",
    "    <p>The output of our dummy model is one-hot-coded tensor. Since there is <b>softmax</b> function in the <b>nn.CrossEntropyLoss</b> function, the loss is:</p>\n",
    "</div>\n",
    "\n",
    "$$loss(x, class) = - \\log \\left( \\frac{\\exp(x[class])}{\\Sigma_j \\exp (x[j])} \\right) = −x[class]+\\log \\left( \\Sigma_j \\exp(x[j]) \\right)$$ \n",
    "\n",
    "and the loss will not be zero.\n",
    "\n",
    "i.e. for $x=[0, 0, 0, 1], class=3$\n",
    "\n",
    "the loss:\n",
    "\n",
    "$$loss(x,class) = -1 +\\log(\\exp(0)+\\exp(0)+\\exp(0)+\\exp(1)) = 0.7437$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To obtain an evaluation accuracy, we can simply compute the average per pixel accuracy of our network for a given image. We will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    test_scores = []\n",
    "    model.eval()\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        targets_mask = targets >= 0\n",
    "        test_scores.append(np.mean((preds.cpu() == targets.cpu())[targets_mask].numpy()))\n",
    "\n",
    "    return np.mean(test_scores)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1,shuffle=False,num_workers=0)\n",
    "print(evaluate_model(dummy_model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You will see reasonably high numbers as your accuracy when you do the training later. The reason behind that is the fact that most output pixels are of a single class and the network can just overfit to common classes such as \"grass\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Design your own model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement your network architecture in <code>exercise_code/networks/segmentation_nn.py</code>. In this task, you will use pytorch to setup your model.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "To compensate for the dimension reduction of a typical convolution layer, you should probably include either a single `nn.Upsample` layer, use a combination of upsampling layers as well as convolutions or even transposed convolutions near the end of your network to get back to the target image shape.\n",
    "\n",
    "This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). \n",
    "The only rules your model design has to follow are:\n",
    "* Inherit from `torch.nn.Module` or `pytorch_lightning.LightningModule`\n",
    "* Perform the forward pass in `forward()`. Input dimension is (N, C, H, W) and output dimension is (N, num_classes, H, W)\n",
    "* Have less than 5 million parameters\n",
    "* Have a model size of less than 50MB after saving\n",
    "\n",
    "Furthermore, you need to pass all your hyperparameters to the model in a single dict `hparams`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Transfer learning</h3>\n",
    "    <p>In this exercise, we encourage you to do transfer learning as we learned in exercise 8, since this will boost your model performance and save training time. You can import pretrained models from torchvision in your model and use its feature extractor (e.g. <code>alexnet.features</code>) to get the image feature. Feel free to choose more advanced pretrained model like ResNet, MobileNet for your architecture design.</p>       \n",
    "</div>\n",
    "\n",
    "See [here](https://pytorch.org/vision/stable/models.html) for more info of the torchvison pretrained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # TODO: if you have any model arguments/hparams, define them here and read them from this dict inside SegmentationNN class\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegmentationNN(hparams = hparams)\n",
    "test_seg_nn(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train your own model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> In addition to the network itself, you will also need to write the code for the model training. You can use PyTorch Lightning for that, or you can also write it yourself in standard PyTorch.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = SegmentationNN(hparams=hparams)\n",
    "########################################################################\n",
    "# TODO - Train Your Model                                              #\n",
    "########################################################################\n",
    "\n",
    "pass\n",
    "\n",
    "#######################################################################\n",
    "#                           END OF YOUR CODE                          #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Test your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(evaluate_model(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualizer(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save the Model for Submission\n",
    "\n",
    "When you are satisfied with your training, save the model for [submission](https://i2dl.vc.in.tum.de/submission/). In order to be eligible for the bonus points you have to achieve an accuracy above __64%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "save_model(model, \"segmentation_nn.model\")\n",
    "checkSize(path = \"./models/segmentation_nn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Congratulations! You've just built your first semantic segmentation model with PyTorch Lightning! To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.vc.in.tum.de/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an ID which we need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.vc.in.tum.de/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted file selectable on the top.\n",
    "3. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement and train a convolutional neural network for Semantic Segmentation.\n",
    "- Passing Criteria: Reach **Accuracy >= 64%** on __our__ test dataset. The submission system will show you your score after you submit.\n",
    "- Submission start: __January 19, 2023 - 13:00__\n",
    "- Submission deadline: __January 25, 2023 - 15:59__\n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Exercise Review](https://docs.google.com/forms/d/e/1FAIpQLScwZArz6ogLqBEj--ItB6unKcv0u9gWLj8bspeiATrDnFH9hA/viewform)\n",
    "\n",
    "We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://docs.google.com/forms/d/e/1FAIpQLScwZArz6ogLqBEj--ItB6unKcv0u9gWLj8bspeiATrDnFH9hA/viewform) for this exercise so that we can do better next time! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
